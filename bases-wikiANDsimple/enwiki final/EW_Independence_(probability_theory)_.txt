
Independence is a fundamental notion in probability theory, as in statistics and the theory of stochastic processes.
Two events are independent, statistically independent, or stochastically independent if the occurrence of one does not affect the probability of occurrence of the other (equivalently, does not affect the odds). Similarly, two random variables are independent if the realization of one does not affect the probability distribution of the other.
When dealing with collections of more than two events, a weak and a strong notion of independence need to be distinguished. The events are called pairwise independent if any two events in the collection are independent of each other, while saying that the events are mutually independent (or collectively independent) intuitively means that each event is independent of any combination of other events in the collection. A similar notion exists for collections of random variables.
The name "mutual independence" (same as "collective independence") seems the outcome of a pedagogical choice, merely to distinguish the stronger notion from "pairwise independence" which is a weaker notion. In the advanced literature of probability theory, statistics, and stochastic processes, the stronger notion is simply named independence with no modifier. It is stronger since independence implies pairwise independence, but not the other way around.
Definition.
For events.
Two events.
Two events formula_1 and formula_2 are independent (often written as formula_3 or formula_4) if and only if their joint probability equals the product of their probabilities:
Why this defines independence is made clear by rewriting with conditional probabilities:
and similarly
Thus, the occurrence of formula_2 does not affect the probability of formula_1, and vice versa. Although the derived expressions may seem more intuitive, they are not the preferred definition, as the conditional probabilities may be undefined if formula_9 or formula_10 are 0. Furthermore, the preferred definition makes clear by symmetry that when formula_1 is independent of formula_2, formula_2 is also independent of formula_1.
Log probability and information content.
Stated in terms of log probability, two events are independent if and only if the log probability of the joint event is the sum of the log probability of the individual events:
In information theory, negative log probability is interpreted as information content, and thus two events are independent if and only if the information content of the combined event equals the sum of information content of the individual events:
See for details.
Odds.
Stated in terms of odds, two events are independent if and only if the odds ratio of and is unity (1). Analogously with probability, this is equivalent to the conditional odds being equal to the unconditional odds:
or to the odds of one event, given the other event, being the same as the odds of the event, given the other event not occurring:
The odds ratio can be defined as
or symmetrically for odds of given , and thus is 1 if and only if the events are independent.
More than two events.
A finite set of events formula_20 is pairwise independent if every pair of events is independent—that is, if and only if for all distinct pairs of indices formula_21,
A finite set of events is mutually independent if every event is independent of any intersection of the other events—that is, if and only if for every formula_22 and for every formula_23-element subset of events formula_24 of formula_20,
This is called the "multiplication rule" for independent events. Note that it is not a single condition involving only the product of all the probabilities of all single events; it must hold true for all subsets of events.
For more than two events, a mutually independent set of events is (by definition) pairwise independent; but the converse is not necessarily true.
For real valued random variables.
Two random variables.
Two random variables formula_26 and formula_27 are independent if and only if (iff) the elements of the π-system generated by them are independent; that is to say, for every formula_28 and formula_29, the events formula_30 and formula_31 are independent events (as defined above in ). That is, formula_26 and formula_27 with cumulative distribution functions formula_34 and formula_35, are independent iff the combined random variable formula_36 has a joint cumulative distribution function
or equivalently, if the probability densities formula_37 and formula_38 and the joint probability density formula_39 exist,
More than two random variables.
A finite set of formula_41 random variables formula_42 is pairwise independent if and only if every pair of random variables is independent. Even if the set of random variables is pairwise independent, it is not necessarily mutually independent as defined next.
A finite set of formula_41 random variables formula_42 is mutually independent if and only if for any sequence of numbers formula_45, the events formula_46 are mutually independent events (as defined above in ). This is equivalent to the following condition on the joint cumulative distribution function A finite set of formula_41 random variables formula_42 is mutually independent if and only if
Notice that it is not necessary here to require that the probability distribution factorizes for all possible subsets as in the case for formula_41 events. This is not required because e.g. formula_50 implies formula_51.
The measure-theoretically inclined may prefer to substitute events formula_52 for events formula_53 in the above definition, where formula_1 is any Borel set. That definition is exactly equivalent to the one above when the values of the random variables are real numbers. It has the advantage of working also for complex-valued random variables or for random variables taking values in any measurable space (which includes topological spaces endowed by appropriate σ-algebras).
For real valued random vectors.
Two random vectors formula_55 and formula_56 are called independent if
where formula_57 and formula_58 denote the cumulative distribution functions of formula_59 and formula_60 and formula_61 denotes their joint cumulative distribution function. Independence of formula_59 and formula_60 is often denoted by formula_64.
Written component-wise, formula_59 and formula_60 are called independent if
For stochastic processes.
For one stochastic process.
The definition of independence may be extended from random vectors to a stochastic process. Therefore, it is required for an independent stochastic process that the random variables obtained by sampling the process at any formula_41 times formula_69 are independent random variables for any formula_41.
Formally, a stochastic process formula_71 is called independent, if and only if for all formula_72 and for all formula_73
where (x_1,\ldots,x_n) = \mathrm{P}(X(t_1) \leq x_1,\ldots,X(t_n) \leq x_n)&lt;/math&gt;.}} Independence of a stochastic process is a property "within" a stochastic process, not between two stochastic processes.
For two stochastic processes.
Independence of two stochastic processes is a property between two stochastic processes formula_71 and formula_75 that are defined on the same probability space formula_76. Formally, two stochastic processes formula_71 and formula_75 are said to be independent if for all formula_72 and for all formula_73, the random vectors formula_81 and formula_82 are independent, i.e. if
Independent σ-algebras.
The definitions above ( and ) are both generalized by the following definition of independence for σ-algebras. Let formula_83 be a probability space and let formula_84 and formula_85 be two sub-σ-algebras of formula_86. formula_84 and formula_85 are said to be independent if, whenever formula_89 and formula_90,
Likewise, a finite family of σ-algebras formula_92, where formula_93 is an index set, is said to be independent if and only if
and an infinite family of σ-algebras is said to be independent if all its finite subfamilies are independent.
The new definition relates to the previous ones very directly:
Using this definition, it is easy to show that if formula_26 and formula_27 are random variables and formula_27 is constant, then formula_26 and formula_27 are independent, since the σ-algebra generated by a constant random variable is the trivial σ-algebra formula_111. Probability zero events cannot affect independence so independence also holds if formula_27 is only Pr-almost surely constant.
Properties.
Self-independence.
Note that an event is independent of itself if and only if
Thus an event is independent of itself if and only if it almost surely occurs or its complement almost surely occurs; this fact is useful when proving zero–one laws.
Expectation and covariance.
If formula_26 and formula_27 are independent random variables, then the expectation operator formula_116 has the property
and the covariance formula_118 is zero, as follows from
The converse does not hold: if two random variables have a covariance of 0 they still may be not independent. See uncorrelated.
Similarly for two stochastic processes formula_71 and formula_75: If they are independent, then they are uncorrelated.
Characteristic function.
Two random variables formula_26 and formula_27 are independent if and only if the characteristic function of the random vector formula_36 satisfies
In particular the characteristic function of their sum is the product of their marginal characteristic functions:
though the reverse implication is not true. Random variables that satisfy the latter condition are called subindependent.
Examples.
Rolling dice.
The event of getting a 6 the first time a die is rolled and the event of getting a 6 the second time are "independent". By contrast, the event of getting a 6 the first time a die is rolled and the event that the sum of the numbers seen on the first and second trial is 8 are "not" independent.
Drawing cards.
If two cards are drawn "with" replacement from a deck of cards, the event of drawing a red card on the first trial and that of drawing a red card on the second trial are "independent". By contrast, if two cards are drawn "without" replacement from a deck of cards, the event of drawing a red card on the first trial and that of drawing a red card on the second trial are "not" independent, because a deck that has had a red card removed has proportionately fewer red cards.
Pairwise and mutual independence.
Consider the two probability spaces shown. In both cases, formula_127 and formula_128. The random variables in the first space are pairwise independent because formula_129, formula_130, and formula_131; but the three random variables are not mutually independent. The random variables in the second space are both pairwise independent and mutually independent. To illustrate the difference, consider conditioning on two events. In the pairwise independent case, although any one event is independent of each of the other two individually, it is not independent of the intersection of the other two:
In the mutually independent case, however,
Mutual independence.
It is possible to create a three-event example in which
and yet no two of the three events are pairwise independent (and hence the set of events are not mutually independent). This example shows that mutual independence involves requirements on the products of probabilities of all combinations of events, not just the single events as in this example.
Conditional independence.
For events.
The events formula_1 and formula_2 are conditionally independent given an event formula_141 when
formula_142.
For random variables.
Intuitively, two random variables formula_26 and formula_27 are conditionally independent given formula_145 if, once formula_145 is known, the value of formula_27 does not add any additional information about formula_26. For instance, two measurements formula_26 and formula_27 of the same underlying quantity formula_145 are not independent, but they are conditionally independent given formula_145 (unless the errors in the two measurements are somehow connected).
The formal definition of conditional independence is based on the idea of conditional distributions. If formula_26, formula_27, and formula_145 are discrete random variables, then we define formula_26 and formula_27 to be "conditionally independent given" formula_145 if
for all formula_28, formula_29 and formula_162 such that formula_163. On the other hand, if the random variables are continuous and have a joint probability density function formula_164, then formula_26 and formula_27 are conditionally independent given formula_145 if
for all real numbers formula_28, formula_29 and formula_162 such that formula_172.
If discrete formula_26 and formula_27 are conditionally independent given formula_145, then
for any formula_28, formula_29 and formula_162 with formula_163. That is, the conditional distribution for formula_26 given formula_27 and formula_145 is the same as that given formula_145 alone. A similar equation holds for the conditional probability density functions in the continuous case.
Independence can be seen as a special kind of conditional independence, since probability can be seen as a kind of conditional probability given no events.

