
In computer science, a trie, also called digital tree or prefix tree, is a type of search tree, a tree data structure used for locating specific keys from within a set. These keys are most often strings, with links between nodes defined not by the entire key, but by individual characters. In order to access a key (to recover its value, change it, or remove it), the trie is traversed depth-first, following the links between nodes, which represent each character in the key. 
Unlike a binary search tree, nodes in the trie do not store their associated key. Instead, a node's position in the trie defines the key with which it is associated. This distributes the value of each key across the data structure, and means that not every node necessarily has an associated key.
All the children of a node have a common prefix of the string associated with that parent node, and the root is associated with the empty string. This task of storing data accessible by its prefix can be accomplished in a memory-optimized way by employing a radix tree. 
Though tries can be keyed by character strings, they need not be. The same algorithms can be adapted for ordered lists of any underlying type, e.g. permutations of digits or shapes. In particular, a bitwise trie is keyed on the individual bits making up a piece of fixed-length binary data, such as an integer or memory address.
History, etymology, and pronunciation.
The idea of a trie for representing a set of strings was first abstractly described by Axel Thue in 1912. Tries were first described in a computer context by René de la Briandais in 1959. The idea was independently described in 1960 by Edward Fredkin, who coined the term "trie", pronouncing it (as "tree"), after the middle syllable of "retrieval". However, other authors pronounce it (as "try"), in an attempt to distinguish it verbally from "tree".
Applications.
Dictionary Representation.
Common applications of tries include storing a predictive text or autocomplete dictionary and implementing approximate matching algorithms, such as those used in spell checking and hyphenation software. Such applications take advantage of a trie's ability to quickly search for, insert, and delete entries. However, if storing dictionary words is all that is required (i.e. there is no need to store metadata associated with each word), a minimal deterministic acyclic finite state automaton (DAFSA) or radix tree would use less storage space than a trie. This is because DAFSAs and radix trees can compress identical branches from the trie which correspond to the same suffixes (or parts) of different words being stored.
Replacing Other Data Structures.
Replacement for Hash Tables.
A trie can be used to replace a hash table, over which it has the following advantages:
However, a trie also has some drawbacks compared to a hash table:
DFSA Representation.
A trie can be seen as a tree-shaped deterministic finite automaton. Each finite language is generated by a trie automaton, and each trie can be compressed into a deterministic acyclic finite state automaton.
Algorithms.
The trie is a tree of nodes which supports Find and Insert operations. Find returns the value for a key string, and Insert inserts a string (the key) and a value into the trie. Both Insert and Find run in time, where m is the length of the key.
A simple Node class can be used to represent nodes in the trie:
class Node:
 def __init__(self) -&gt; None:
 # Note that using a dictionary for children (as in this implementation)
 # would not by default lexicographically sort the children, which is
 # required by the lexicographic sorting in the Sorting section.
 # For lexicographic sorting, we can instead use an array of Nodes.
 self.children: Dict[str, Node] = {} # mapping from character to Node
 self.value: Optional[Any] = None
Note that codice_1 is a dictionary of characters to a node's children; and it is said that a "terminal" node is one which represents a complete string.&lt;br&gt;A trie's value can be looked up as follows:
def find(node: Node, key: str) -&gt; Optional[Any]:
 """Find value by key in node."""
 for char in key:
 if char in node.children:
 node = node.children[char]
 else:
 return None
 return node.value
A slight modifications of this routine can be utilized
Insertion proceeds by walking the trie according to the string to be inserted, then appending new nodes for the suffix of the string that is not contained in the trie:
def insert(node: Node, key: str, value: Any) -&gt; None:
 """Insert key/value pair into node."""
 for char in key:
 if char not in node.children:
 node.children[char] = Node()
 node = node.children[char]
 node.value = value
Deletion of a key can be done lazily (by clearing just the value within the node corresponding to a key), or eagerly by cleaning up any parent nodes that are no longer necessary. Eager deletion is described in the pseudocode here:
def delete(root: Node, key: str) -&gt; bool:
 """Eagerly delete the key from the trie rooted at `root`.
 Return whether the trie rooted at `root` is now empty.
 
 def _delete(node: Node, key: str, d: int) -&gt; bool:
 """Clear the node corresponding to key[d], and delete the child key[d+1]
 if that subtrie is completely empty, and return whether `node` has been
 cleared.
 if d == len(key):
 node.value = None
 else:
 c = key[d]
 if c in node.children and _delete(node.children[c], key, d+1):
 del node.children[c]
 # Return whether the subtrie rooted at `node` is now completely empty
 return node.value is None and len(node.children) == 0
 return _delete(root, key, 0)
Autocomplete.
Tries can be used to return a list of keys with a given prefix. This can also be modified to allow for wildcards in the prefix search.
def keys_with_prefix(root: Node, prefix: str) -&gt; List[str]:
 results: List[str] = []
 x = _get_node(root, prefix)
 _collect(x, list(prefix), results)
 return results
def _collect(x: Optional[Node], prefix: List[str], results: List[str]) -&gt; None:
 Append keys under node `x` matching the given prefix to `results`.
 prefix: list of characters
 if x is None:
 return
 if x.value is not None:
 prefix_str = ".join(prefix)
 results.append(prefix_str)
 for c in x.children:
 prefix.append(c)
 _collect(x.children[c], prefix, results)
 del prefix[-1] # delete last character
def _get_node(node: Node, key: str) -&gt; Optional[Node]:
 Find node by key. This is the same as the `find` function defined above,
 but returning the found node itself rather than the found node's value.
 for char in key:
 if char in node.children:
 node = node.children[char]
 else:
 return None
 return node
Sorting.
Lexicographic sorting of a set of keys can be accomplished by building a trie from them, with the children of each node sorted lexicographically, and traversing it in pre-order, printing any values in either the interior nodes or in the leaf nodes. This algorithm is a form of radix sort.
A trie is the fundamental data structure of Burstsort, which (in 2007) was the fastest known string sorting algorithm due to its efficient cache use. Now there are faster ones.
Full-text search.
A special kind of trie, called a suffix tree, can be used to index all suffixes in a text in order to carry out fast full text searches.
Implementation strategies.
There are several ways to represent tries, corresponding to different trade-offs between memory use and speed of the operations. The basic form is that of a linked set of nodes, where each node contains an array of child pointers, one for each symbol in the alphabet (so for the English alphabet, one would store 26 child pointers and for the alphabet of bytes, 256 pointers). This is simple but wasteful in terms of memory: using the alphabet of bytes (size 256) and four-byte pointers, each node requires a kilobyte of storage, and when there is little overlap in the strings' prefixes, the number of required nodes is roughly the combined length of the stored strings. Put another way, the nodes near the bottom of the tree tend to have few children and there are many of them, so the structure wastes space storing null pointers.
The storage problem can be alleviated by an implementation technique called "alphabet reduction", whereby the original strings are reinterpreted as longer strings over a smaller alphabet. E.g., a string of bytes can alternatively be regarded as a string of four-bit units and stored in a trie with sixteen pointers per node. Lookups need to visit twice as many nodes in the worst case, but the storage requirements go down by a factor of eight.
An alternative implementation represents a node as a triple and links the children of a node together as a singly linked list: points to the node's first child, to the parent node's next child. The set of children can also be represented as a binary search tree; one instance of this idea is the ternary search tree developed by Bentley and Sedgewick.
Another alternative in order to avoid the use of an array of 256 pointers (ASCII), as suggested before, is to store the alphabet array as a bitmap of 256 bits representing the ASCII alphabet, reducing dramatically the size of the nodes.
Bitwise tries.
Bitwise tries are much the same as a normal character-based trie except that individual bits are used to traverse what effectively becomes a form of binary tree. Generally, implementations use a special CPU instruction to very quickly find the first set bit in a fixed length key (e.g., GCC's codice_2 intrinsic). This value is then used to index a 32- or 64-entry table which points to the first item in the bitwise trie with that number of leading zero bits. The search then proceeds by testing each subsequent bit in the key and choosing codice_3 or codice_4 appropriately until the item is found.
Although this process might sound slow, it is very cache-local and highly parallelizable due to the lack of register dependencies and therefore in fact has excellent performance on modern out-of-order execution CPUs. A red–black tree for example performs much better on paper, but is highly cache-unfriendly and causes multiple pipeline and TLB stalls on modern CPUs which makes that algorithm bound by memory latency rather than CPU speed. In comparison, a bitwise trie rarely accesses memory, and when it does, it does so only to read, thus avoiding SMP cache coherency overhead. Hence, it is increasingly becoming the algorithm of choice for code that performs many rapid insertions and deletions, such as memory allocators (e.g., recent versions of the famous Doug Lea's allocator (dlmalloc) and its descendants). The worst case of steps for lookup is the same as bits used to index bins in the tree.
Alternatively, the term "bitwise trie" can more generally refer to a binary tree structure holding integer values, sorting them by their binary prefix. An example is the x-fast trie.
Compressing tries.
Compressing the trie and merging the common branches can sometimes yield large performance gains. This works best under the following conditions:
For example, it may be used to represent sparse bitsets; i.e., subsets of a much larger, fixed enumerable set. In such a case, the trie is keyed by the bit element position within the full set. The key is created from the string of bits needed to encode the integral position of each element. Such tries have a very degenerate form with many missing branches. After detecting the repetition of common patterns or filling the unused gaps, the unique leaf nodes (bit strings) can be stored and compressed easily, reducing the overall size of the trie.
Such compression is also used in the implementation of the various fast lookup tables for retrieving Unicode character properties. These could include case-mapping tables (e.g., for the Greek letter pi, from Π to π), or lookup tables normalizing the combination of base and combining characters (like the a-umlaut in German, ä, or the dalet-patah-dagesh-ole in Biblical Hebrew, ). For such applications, the representation is similar to transforming a very large, unidimensional, sparse table (e.g., Unicode code points) into a multidimensional matrix of their combinations, and then using the coordinates in the hyper-matrix as the string key of an uncompressed trie to represent the resulting character. The compression will then consist of detecting and merging the common columns within the hyper-matrix to compress the last dimension in the key. For example, to avoid storing the full, multibyte Unicode code point of each element forming a matrix column, the groupings of similar code points can be exploited. Each dimension of the hyper-matrix stores the start position of the next dimension, so that only the offset (typically a single byte) need be stored. The resulting vector is itself compressible when it is also sparse, so each dimension (associated to a layer level in the trie) can be compressed separately.
Some implementations do support such data compression within dynamic sparse tries and allow insertions and deletions in compressed tries. However, this usually has a significant cost when compressed segments need to be split or merged. Some tradeoff has to be made between data compression and update speed. A typical strategy is to limit the range of global lookups for comparing the common branches in the sparse trie.
The result of such compression may look similar to trying to transform the trie into a directed acyclic graph (DAG), because the reverse transform from a DAG to a trie is obvious and always possible. However, the shape of the DAG is determined by the form of the key chosen to index the nodes, in turn constraining the compression possible.
Another compression strategy is to "unravel" the data structure into a single byte array.
This approach eliminates the need for node pointers, substantially reducing the memory requirements. This in turn permits memory mapping and the use of virtual memory to efficiently load the data from disk.
One more approach is to "pack" the trie. Liang describes a space-efficient implementation of a sparse packed trie applied to automatic hyphenation, in which the descendants of each node may be interleaved in memory.
External memory tries.
Several trie variants are suitable for maintaining sets of strings in external memory, including suffix trees. A combination of trie and B-tree, called the "B-trie" has also been suggested for this task; compared to suffix trees, they are limited in the supported operations but also more compact, while performing update operations faster.

