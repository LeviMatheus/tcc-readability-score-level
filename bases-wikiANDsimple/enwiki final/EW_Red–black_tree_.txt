
In computer science, a red–black tree is a kind of self-balancing binary search tree. Each node stores an extra bit representing "color" ("red" or "black"), used to ensure that the tree remains balanced during insertions and deletions.
When the tree is modified, the new tree is rearranged and "repainted" to restore the coloring properties that constrain how unbalanced the tree can become in the worst case. The properties are designed such that this rearranging and recoloring can be performed efficiently.
The re-balancing is not perfect, but guarantees searching in formula_1 time, where formula_2 is the number of nodes of the tree. The insertion and deletion operations, along with the tree rearrangement and recoloring, are also performed in formula_1 time.
Tracking the color of each node requires only 1 bit of information per node because there are only two colors. The tree does not contain any other data specific to its being a red–black tree so its memory footprint is almost identical to a classic (uncolored) binary search tree. In many cases, the additional bit of information can be stored at no additional memory cost.
History.
In 1972, Rudolf Bayer invented a data structure that was a special order-4 case of a B-tree. These trees maintained all paths from root to leaf with the same number of nodes, creating perfectly balanced trees. However, they were not "binary" search trees. Bayer called them a "symmetric binary B-tree" in his paper and later they became popular as 2–3–4 trees or just 2–4 trees.
In a 1978 paper, "A Dichromatic Framework for Balanced Trees", Leonidas J. Guibas and Robert Sedgewick derived the red–black tree from the symmetric binary B-tree. The color "red" was chosen because it was the best-looking color produced by the color laser printer available to the authors while working at Xerox PARC. Another response from Guibas states that it was because of the red and black pens available to them to draw the trees.
In 1993, Arne Andersson introduced the idea of a right leaning tree to simplify insert and delete operations.
In 1999, Chris Okasaki showed how to make the insert operation purely functional. Its balance function needed to take care of only 4 unbalanced cases and one default balanced case.
The original algorithm used 8 unbalanced cases, but reduced that to 6 unbalanced cases. Sedgewick showed that the insert operation can be implemented in just 46 lines of Java code.
In 2008, Sedgewick proposed the left-leaning red–black tree, leveraging Andersson’s idea that simplified the insert and delete operations. Sedgewick originally allowed nodes whose two children are red, making his trees more like 2–3–4 trees, but later this restriction was added, making new trees more like 2–3 trees. Sedgewick implemented the insert algorithm in just 33 lines, significantly shortening his original 46 lines of code.
Terminology.
A red–black tree is a special type of binary search tree, used in computer science to organize pieces of comparable data, such as text fragments or numbers (as e.g. the numbers in figures 1 and 2). The nodes carrying keys and/or data are frequently called "internal nodes", but in order to make this very specific they are also called non-NIL nodes in this article.
The leaf nodes of red–black trees ( NIL  in figure 1) do not contain keys or data. These "leaves" need not be explicit individuals in computer memory: a NULL pointer can —as in all binary tree data structures— encode the fact that there is no child node at this position in the (parent) node. Nevertheless, by their position in the tree, these objects are in relation to other nodes which is relevant to the RB-structure, it may have parent, sibling (i.e., the other child of the parent), uncle, even nephew node; and may be child—but never parent of another node.
It is not really necessary to attribute a "color" to these end-of-path objects, because the condition "is codice_1 or codice_2" is implied by the condition "is codice_1".
Figure 2 shows the conceptually same red–black tree without these NIL leaves. In order to arrive at the same notion of a path, one has to notice that e.g. 3 paths run through the node 1, namely a path through 1left plus 2 additional paths through 1right, namely the paths through 6left and 6right. This way, these ends of the paths are also docking points for new nodes to be inserted, fully equivalent to the NIL leaves of figure 1.
On the other hand, in order to save a marginal amount of execution time, these (possibly many) NIL leaves may be implemented as pointers to one unique (and black) sentinel node (instead of pointers of value NULL).
As a conclusion, the fact that a child does not exist (is not a true node, does not contain data) can in all occurrences be specified by the very same NULL pointer or as the very same pointer to a sentinel node. Throughout this article, either choice of this value will be denoted by the constant named codice_1.
The black depth of a node is defined as the number of black nodes from the root to that node (i.e. the number of black ancestors). The black height of a red–black tree is the number of black nodes in any path from the root to the leaves, which, by property 4, is constant (alternatively, it could be defined as the black depth of any leaf node).
The black height of a node is the black height of the subtree rooted by it. In this article, the black height of a NIL leaf shall be set to 0, because its subtree is empty as suggested by figure 2, and its tree height is also 0.
Properties.
In addition to the requirements imposed on a binary search tree the following must be satisfied by a 
Some authors, e.g. Cormen &amp; al., claim "the root is black" as fifth property; but not Mehlhorn &amp; Sanders or Sedgewick &amp; Wayne. Since the root can always be changed from red to black, this rule has little effect on analysis.
This article also omits it, because it slightly disturbs the recursive algorithms and proofs.
As an example, every perfect binary tree that consists only of black nodes is a red–black tree.
The read-only operations, such as search or tree traversal, do not affect any of the properties. On the other hand, the modifying operations insert and easily maintain properties 1 and 2, but with respect to the other properties some extra effort has to be taken, in order to avoid the introduction of a violation of property 3, called a red-violation, or of property 4, called a black-violation.
The requirements enforce a critical property of red–black trees: "the path from the root to the farthest leaf is no more than twice as long as the path from the root to the nearest leaf". The result is that the tree is height-balanced. Since operations such as inserting, deleting, and finding values require worst-case time proportional to the height formula_4 of the tree, this upper bound on the height allows red–black trees to be efficient in the worst case, namely logarithmic in the number formula_2 of nodes, i.e. formula_6, which is not the case for ordinary binary search trees. For a mathematical proof see section Proof of bounds.
Red–black trees, like all binary search trees, allow quite efficient sequential access (e.g. in-order traversal, that is: in the order Left–Root–Right) of their elements. But they support also asymptotically optimal direct access via a traversal from root to leaf, resulting in formula_1 search time.
Analogy to B-trees of order 4.
A red–black tree is similar in structure to a B-tree of order 4, where each node can contain between 1 and 3 values and (accordingly) between 2 and 4 child pointers. In such a B-tree, each node will contain only one value matching the value in a black node of the red–black tree, with an optional value before and/or after it in the same node, both matching an equivalent red node of the red–black tree.
One way to see this equivalence is to "move up" the red nodes in a graphical representation of the red–black tree, so that they align horizontally with their parent black node, by creating together a horizontal cluster. In the B-tree, or in the modified graphical representation of the red–black tree, all leaf nodes are at the same depth.
The red–black tree is then structurally equivalent to a B-tree of order 4, with a minimum fill factor of 33% of values per cluster with a maximum capacity of 3 values.
This B-tree type is still more general than a red–black tree though, as it allows ambiguity in a red–black tree conversion—multiple red–black trees can be produced from an equivalent B-tree of order 4 (see figure 3). If a B-tree cluster contains only 1 value, it is the minimum, black, and has two child pointers. If a cluster contains 3 values, then the central value will be black and each value stored on its sides will be red. If the cluster contains two values, however, either one can become the black node in the red–black tree (and the other one will be red).
So the order-4 B-tree does not maintain which of the values contained in each cluster is the root black tree for the whole cluster and the parent of the other values in the same cluster. Despite this, the operations on red–black trees are more economical in time because you don’t have to maintain the vector of values. It may be costly if values are stored directly in each node rather than being stored by reference. B-tree nodes, however, are more economical in space because you don’t need to store the color attribute for each node. Instead, you have to know which slot in the cluster vector is used. If values are stored by reference, e.g. objects, null references can be used and so the cluster can be represented by a vector containing 3 slots for value pointers plus 4 slots for child references in the tree. In that case, the B-tree can be more compact in memory, improving data locality.
The same analogy can be made with B-trees with larger orders that can be structurally equivalent to a colored binary tree: you just need more colors. Suppose that you add blue, then the blue–red–black tree defined like red–black trees but with the additional constraint that no two successive nodes in the hierarchy will be blue and all blue nodes will be children of a red node, then it becomes equivalent to a B-tree whose clusters will have at most 7 values in the following colors: blue, red, blue, black, blue, red, blue (For each cluster, there will be at most 1 black node, 2 red nodes, and 4 blue nodes).
For moderate volumes of values, insertions and deletions in a colored binary tree are faster compared to B-trees because colored trees don’t attempt to maximize the fill factor of each horizontal cluster of nodes (only the minimum fill factor is guaranteed in colored binary trees, limiting the number of splits or junctions of clusters). B-trees will be faster for performing rotations (because rotations will frequently occur within the same cluster rather than with multiple separate nodes in a colored binary tree). For storing large volumes, however, B-trees will be much faster as they will be more compact by grouping several children in the same cluster where they can be accessed locally.
All optimizations possible in B-trees to increase the average fill factors of clusters are possible in the equivalent multicolored binary tree. Notably, maximizing the average fill factor in a structurally equivalent B-tree is the same as reducing the total height of the multicolored tree, by increasing the number of non-black nodes. The worst case occurs when all nodes in a colored binary tree are black, the best case occurs when only a third of them are black (and the other two thirds are red nodes).
Applications and related data structures.
Red–black trees offer worst-case guarantees for insertion time, deletion time, and search time. Not only does this make them valuable in time-sensitive applications such as real-time applications, but it makes them valuable building blocks in other data structures which provide worst-case guarantees; for example, many data structures used in computational geometry can be based on red–black trees, and the Completely Fair Scheduler used in current Linux kernels and epoll system call implementation uses red–black trees.
The AVL tree is another structure supporting formula_1 search, insertion, and removal. AVL trees can be colored red–black, thus are a subset of RB trees. Worst-case height is 0.720 times the worst-case height of RB trees, so AVL trees are more rigidly balanced. The performance measurements of Ben Pfaff with realistic test cases in 79 runs find AVL to RB ratios between 0.677 and 1.077, median at 0.947, and geometric mean 0.910. WAVL trees have a performance in between those two.
Red–black trees are also particularly valuable in functional programming, where they are one of the most common persistent data structures, used to construct associative arrays and sets which can retain previous versions after mutations. The persistent version of red–black trees requires formula_1 space for each insertion or deletion, in addition to time.
For every 2–4 tree, there are corresponding red–black trees with data elements in the same order. The insertion and deletion operations on 2–4 trees are also equivalent to color-flipping and rotations in red–black trees. This makes 2–4 trees an important tool for understanding the logic behind red–black trees, and this is why many introductory algorithm texts introduce 2–4 trees just before red–black trees, even though 2–4 trees are not often used in practice.
In 2008, Sedgewick introduced a simpler version of the red–black tree called the left-leaning red–black tree by eliminating a previously unspecified degree of freedom in the implementation. The LLRB maintains an additional invariant that all red links must lean left except during inserts and deletes. Red–black trees can be made isometric to either 2–3 trees, or 2–4 trees, for any sequence of operations. The 2–4 tree isometry was described in 1978 by Sedgewick. With 2–4 trees, the isometry is resolved by a "color flip," corresponding to a split, in which the red color of two children nodes leaves the children and moves to the parent node.
The original description of the tango tree, a type of tree optimized for fast searches, specifically uses red–black trees as part of its data structure.
As of Java 8, the HashMap has been modified such that instead of using a LinkedList to store different elements with colliding hashcodes, a red–black tree is used. This results in the improvement of time complexity of searching such an element from formula_10 to formula_1.
Operations.
The read-only operations, such as search or tree traversal, on a red–black tree require no modification from those used for binary search trees, because every red–black tree is a special case of a simple binary search tree. However, the immediate result of an insertion or removal may violate the properties of a red–black tree, the restoration of which is called rebalancing so that red–black trees become self-balancing.
It requires maximally a small number, formula_1 in Big O notation, or on average an amortized constant number of color changes (which are very quick in practice); and no more than three tree rotations (two for insertion). Although insert and delete operations are complicated, their times remain formula_1, where formula_2 is the number of objects in the tree.
If the example implementation below is not suitable, other implementations with explanations may be found in Ben Pfaff’s annotated C library GNU libavl (v2.0.3 as of June 2019).
The details of the insert and removal operations will be demonstrated with example C++ code. The example code may call upon the helper functions below to find the parent, grandparent, uncle, sibling and nephew nodes and to rotate a subtree left or right:
// Basic type definitions:
enum color_t { BLACK, RED };
struct RBnode { // node of red–black tree
 RBnode* parent; // == NULL if root of the tree
 RBnode* child[2]; // == NIL if child is empty
 // Index is:
 // LEFT := 0, if (key &lt; parent-&gt;key)
 // RIGHT := 1, if (key &gt; parent-&gt;key)
 enum color_t color;
 int key;
struct RBtree { // red–black tree
 RBnode* root; // == NIL if tree is empty
// Get the child direction (∈ { LEFT, RIGHT })
// of the non-root non-NIL RBnode* N:
// Helper functions:
RBnode* GetParent(RBnode* N) {
 // The parent of the root node is set to NULL.
 return N == NULL ? NULL : N-&gt;parent;
RBnode* GetGrandParent(RBnode* N) {
 // Will return NULL if N is root or child of root
 return GetParent(GetParent(N));
RBnode* GetSibling(RBnode* N) {
 RBnode* P = GetParent(N);
 // No parent means no sibling:
 assert(P != NULL);
 return P-&gt;child[1-childDir(N)];
// If parent P and child direction dir is available, same as:
// P-&gt;child[1-dir]
RBnode* GetUncle(RBnode* N) {
 RBnode* P = GetParent(N);
 // No parent means no uncle:
 assert(P != NULL);
 return GetSibling(P);
RBnode* GetCloseNephew(RBnode* N) {
 RBnode* P = GetParent(N);
 int dir;
 RBnode* S;
 assert(P != NULL);
 dir = childDir(N);
 S = P-&gt;child[1-dir]; // sibling of N
 assert(S != NIL);
 return S-&gt;child[dir]; // the nephew close to N
RBnode* GetDistantNephew(RBnode* N) {
 RBnode* P = GetParent(N);
 int dir;
 RBnode* S;
 assert(P != NULL);
 dir = childDir(N);
 S = P-&gt;child[1-dir]; // sibling of N
 assert(S != NIL);
 return S-&gt;child[1-dir]; // the nephew distant from N
RBnode* RotateDirRoot(
 RBtree* T, // red–black tree
 RBnode* P, // root of subtree (may be the root of T)
 RBnode* G = P-&gt;parent;
 RBnode* S = P-&gt;child[1-dir];
 RBnode* C;
 assert(S != NIL); // pointer to true node required
 C = S-&gt;child[dir];
 P-&gt;child[1-dir] = C; if (C != NIL) C-&gt;parent = P;
 S-&gt;child[ dir] = P; P-&gt;parent = S;
 S-&gt;parent = G;
 if (G != NULL)
 G-&gt;child[ P == G-&gt;right ? RIGHT : LEFT ] = S;
 else
 T-&gt;root = S;
 return S; // new root of subtree
The proposal breaks down both, insertion and removal (not mentioning some very simple cases), into six configurations of nodes, edges and colors, which are called cases. The code repairing (rebalancing) a case sometimes uses code of a subsequent case. The proposal contains for both, insertion and removal, exactly one case which advances one black level closer to the root and loops, the other five cases rebalance the tree of their own. The more complicated cases are pictured in a diagram.
Insertion.
Insertion begins by placing the new (non-NIL) node, say N, at the position in the binary search tree of a NIL leaf whose in-order predecessor’s key compares less than the new node’s key which in turn compares less than the key of its in-order successor.
(Frequently, this positioning is the result of a search within the tree immediately preceding the insert operation and consists of a node codice_6 together with a direction codice_7 with 
The newly inserted node is temporarily colored red so that all paths contain the same number of black nodes as before.
But if its parent, say P, is also red then this action introduces a red-violation.
void RBinsert1(
 RBtree* T, // -&gt; red–black tree
 struct RBnode* P, // -&gt; parent node of N (may be NULL)
 struct RBnode* N, // -&gt; node to be inserted
 byte dir) // side of P on which to insert N (∈ { LEFT, RIGHT })
 struct RBnode* G; // -&gt; parent node of P
 struct RBnode* U; // -&gt; uncle of N
 N-&gt;color = RED;
 N-&gt;left = NIL;
 N-&gt;right = NIL;
 N-&gt;parent = P;
 if (P == NULL) { // There is no parent
 T-&gt;root = N; // N is the new root of the tree T.
 return; // insertion complete
 P-&gt;child[dir] = N; // insert N as dir-child of P
 // fall through to the loop
The rebalancing loop has the following invariant:
Insert case 3: The current node’s parent P is black, so property 3 (both children of every red node are black) holds. Property 4 holds as well according to the loop invariant.
 // start of the (do while)-loop:
 do {
 if (P-&gt;color == BLACK) {
 // I_case_3 (P black):
 return; // insertion complete
 // From now on P is red.
 if ((G = GetParent(P)) == NULL) 
 goto I_case_6; // P red and root
 // else: P red and G!=NULL.
 dir = childDir(P); // the side of parent G on which node P is located
 U = G-&gt;child[1-dir]; // uncle
 if (U == NIL || U-&gt;color == BLACK) // considered black
 goto I_case_45; // P red &amp;&amp; U black
Insert case 1: If both the parent P and the uncle U are red, then both of them can be repainted black and the grandparent G becomes red for maintaining property 4 (all paths from a node to the leaves contain the same number of black nodes). Since any path through the parent or uncle must pass through the grandparent, the number of black nodes on these paths has not changed. However, the grandparent G may now violate property 3 (Both children of every red node are black) if it has a red parent. After relabeling G to N the loop invariant is fulfilled so that the rebalancing procedure can be repeated on one black level (= 2 tree levels) higher.
 // I_case_1 (P+U red):
 P-&gt;color = BLACK;
 U-&gt;color = BLACK;
 G-&gt;color = RED;
 N = G; // new current node
 // iterate 1 black level (= 2 tree levels) higher
 } while ((P = N-&gt;parent) != NULL); // end of (do while)-loop
 // fall through to I_case_2
Insert case 2: The current node N is the root of the tree. Then all properties are satisfied with the red root N.
 // I_case_2 (P == NULL):
 return; // insertion complete
Insert case 4: The parent P is red but the uncle U is black. The ultimate goal is to rotate the parent node P to the grandparent position, but this will not work if N is an "inner" grandchild of G (i.e., if N is the left child of the right child of G or the right child of the left child of G). A at P switches the roles of the current node N and its parent P. The rotation adds paths through N (those in the subtree labeled 2, see diagram) and removes paths through P (those in the subtree labeled 4). But both P and N are red, so property 4 (all paths from a node to its leaves contain the same number of black nodes) is preserved. Property 3 (both children of every red node are black) is restored in case 5.
I_case_45: // P red &amp;&amp; U black:
 if (N == P-&gt;child[1-dir]) {
 // I_case_4 (P red &amp;&amp; U black &amp;&amp; N inner grandchild of G):
 RotateDir(P,dir); // P is never the root
 N = P; // new current node
 P = G-&gt;child[dir]; // new parent of N
 // fall through to I_case_5
Insert case 5: The current node N is now certain to be an "outer" grandchild of G (left of left child or right of right child). Now at G, putting P in place of G and making P the parent of N and G. G is black and its former child P is red, since property 3 was violated. After switching the colors of P and G the resulting tree satisfies property 3 (a red node has black children). Property 4 (all paths from a node to its leaves contain the same number of black nodes) also remains satisfied, since all paths that went through the black G now go through the black P.
 // I_case_5 (P red &amp;&amp; U black &amp;&amp; N outer grandchild of G):
 RotateDirRoot(T,G,1-dir); // G may be the root
 P-&gt;color = BLACK;
 G-&gt;color = RED;
 return; // insertion complete
Insert case 6: The parent P is red and the root.
Because N is also red property 3 (a red node has black children) is violated. But after switching P’s color the tree is in RB-shape.
The black height of the tree increases by 1.
I_case_6: // P is the root and red:
 P-&gt;color = BLACK;
 return; // insertion complete
} // end of RBinsert1
Because the algorithm transforms the input without using an auxiliary data structure and using only a small amount of extra storage space for auxiliary variables it is in-place.
Rotations occur in the cases 4 and 5, all outside the loop.
Therefore, at most two rotations occur in total.
In the algorithm above, all cases are executed only once, except case 1 where the iterative implementation effectively loops with the grandparent node as current. Because the problem of repair in that case is escalated two levels higher each time, it takes maximally formula_15 iterations to repair the tree (where formula_4 is the height of the tree). Because the probability for escalation decreases exponentially with each iteration the insertion cost is constant on average, indeed amortized constant.
Removal: simple cases.
The label N denotes the current node which at entry is the node to be deleted.
If N is the root which does not have a non-NIL child, it is replaced by a NIL leaf, after which the tree is empty—and in RB-shape.
If N has two non-NIL children, an additional navigation to either the maximum element in its left subtree (which is the in-order predecessor) or the minimum element in its right subtree (which is the in-order successor) finds a node with no other node in between (as shown here). Without touching the user data of this node, all red–black tree pointers of this node and N are exchanged so that N now has at most one non-NIL child.
If N has exactly one non-NIL child, it must be a red child, because if it were a black one then property 4 would force a second black non-NIL child.
If N is a red node, it cannot have a non-NIL child, because this would have to be black by property 3. Moreover, it cannot have exactly one black child as argued just above. As a consequence the red node N is without any child and can simply be removed.
If N is a black node, it may have a red child or no non-NIL child at all.
If N has a red child, it is simply replaced with this child after painting the latter black.
Removal of a black non-root leaf.
The complex case is when N is not the root, colored black and does not have a non-NIL child.
In the first iteration, N is replaced by NIL.
void RBdelete2(
 RBtree* T, // -&gt; red–black tree
 struct RBnode* N) // -&gt; node to be deleted
 struct RBnode* P = N-&gt;parent; // -&gt; parent node of N
 byte dir; // side of P on which N is located (∈ { LEFT, RIGHT })
 struct RBnode* S; // -&gt; sibling of N
 struct RBnode* C; // -&gt; close nephew
 struct RBnode* D; // -&gt; distant nephew
 // P != NULL, since N is not the root.
 dir = childDir(N); // side of parent P on which the node N is located
 // Replace N at its parent P by NIL:
 P-&gt;child[dir] = NIL;
 goto Start_D; // jump into the loop
The rebalancing loop has the following invariant:
 // start of the (do while)-loop:
 do {
 dir = childDir(N); // side of parent P on which node N is located
Start_D:
 S = P-&gt;child[1-dir]; // sibling of N (has black height &gt;= 1)
 if (S-&gt;color == RED)
 goto D_case_3; // S red ===&gt; P+C+D black
 // S is black:
 D = S-&gt;child[1-dir]; // distant nephew
 if (D != NIL &amp;&amp; D-&gt;color == RED) // not considered black
 goto D_case_6; // D red &amp;&amp; S black
 C = S-&gt;child[ dir]; // close nephew
 if (C != NIL &amp;&amp; C-&gt;color == RED) // not considered black
 goto D_case_5; // C red &amp;&amp; S+D black
 // Here both nephews are == NIL (first iteration) or black (later).
 if (P-&gt;color == RED)
 goto D_case_4; // P red &amp;&amp; C+S+D black
 // P+C+S+D black: fall through to D_case_1
Delete case 1: P, S, and S’s children are black. After painting S red all paths passing through S, which are precisely those paths "not" passing through N, have one less black node. Now all paths in the subtree rooted by P have the same number of black nodes, but one fewer than the paths that do not pass through P, so property 4 (all paths from any given node to its leaf nodes contain the same number of black nodes) may still be violated. After relabeling P to N the loop invariant is fulfilled so that the rebalancing procedure can be repeated on one black level (= 1 tree level) higher.
 // D_case_1 (P+C+S+D black):
 S-&gt;color = RED;
 N = P; // new current node (maybe the root)
 // iterate 1 black level (= 1 tree level) higher
 } while ((P = N-&gt;parent) != NULL); // end of (do while)-loop
Delete case 2: The current node N is the new root. One black node has been removed from every path, so the RB-properties are preserved.
The black height of the tree decreases by 1.
 // D_case_2 (P == NULL):
 return; // deletion complete
Delete case 3: The sibling S is red, so P and the nephews C and D have to be black. A at P turns S into N’s grandparent.
Then after reversing the colors of P and S, the path through N is still short one black node. But N has a red parent and a black sibling, so the transformations in cases 4, 5, or 6 are able to restore the RB-shape.
D_case_3: // S red &amp;&amp; P+C+D black:
 RotateDirRoot(T,P,dir); // P may be the root
 P-&gt;color = RED;
 S-&gt;color = BLACK;
 S = C; // != NIL
 // now: P red &amp;&amp; S black
 // fall through
Delete case 4: The sibling S and S’s children are black, but P is red. Exchanging the colors of S and P does not affect the number of black nodes on paths going through S, but it does add one to the number of black nodes on paths going through N, making up for the deleted black node on those paths.
 D = S-&gt;child[1-dir]; // distant nephew
 if (D != NIL &amp;&amp; D-&gt;color == RED) // not considered black
 goto D_case_6; // D red &amp;&amp; S black
 C = S-&gt;child[ dir]; // close nephew
 if (C != NIL &amp;&amp; C-&gt;color == RED) // not considered black
 goto D_case_5; // C red &amp;&amp; S+D black
D_case_4: // P red &amp;&amp; S black &amp;&amp; C+D considered black:
 S-&gt;color = RED;
 P-&gt;color = BLACK;
 return; // deletion complete
Delete case 5: The sibling S is black, S’s close child C is red, and S’s distant child D is black. After a at S the nephew C becomes S’s parent and N’s new sibling. The colors of S and C are exchanged.
All paths still have the same number of black nodes, but now N has a black sibling whose distant child is red, so the configuration is fit for case 6. Neither N nor its parent P are affected by this transformation, and P may be red or black .
D_case_5: // C red &amp;&amp; S+D black:
 RotateDir(S,1-dir); // S is never the root
 S-&gt;color = RED;
 C-&gt;color = BLACK;
 D = S;
 S = C;
 // now: D red &amp;&amp; S black
 // fall through to D_case_6
Delete case 6: The sibling S is black, S’s distant child D is red. After a at P the sibling S becomes the parent of P and S’s distant child D. The colors of P and S are exchanged, and D is made black. The subtree still has the same color at its root, namely either red or black ( in the diagram), which refers to the same color both before and after the transformation. This way property 3 (Both children of every red node are black) is preserved. The paths in the subtree not passing through N (i.o.w. passing through D and node 3 in the diagram) pass through the same number of black nodes as before, but N now has one additional black ancestor: either P has become black, or it was black and S was added as a black grandparent. Thus, the paths passing through N pass through one additional black node, so that property 4 (All paths from any given node to its leaf nodes contain the same number of black nodes) is restored and the total tree is in RB-shape.
D_case_6: // D red &amp;&amp; S black:
 RotateDirRoot(T,P,dir); // P may be the root
 S-&gt;color = P-&gt;color;
 P-&gt;color = BLACK;
 D-&gt;color = BLACK;
 return; // deletion complete
} // end of RBdelete2
Because the algorithm transforms the input without using an auxiliary data structure and using only a small amount of extra storage space for auxiliary variables it is in-place.
Rotations occur in the cases 3, 5 and 6, all outside the loop.
Therefore, at most three rotations occur in total.
In the algorithm above, all cases are chained in order, except in case 1, which is the only case where the iterative implementation effectively loops. The problem of repair in that case is escalated one level higher each time. Thus, no more than formula_4 iterations will occur (where formula_4 is the height of the tree).
Because the probability for escalation decreases exponentially with each iteration the removal cost is constant on average, indeed amortized constant.
Mehlhorn &amp; Sanders point out: "AVL trees do not support constant "amortized" deletion costs", but red–black trees do.
Proof of bounds.
For formula_19 there is a red–black tree of height formula_4 with
and there is no red–black tree of this height with fewer nodes—therefore it is minimal.Its black height is  formula_22
For a red–black tree of a certain height to have minimal number of nodes, it has to have exactly one longest path with maximal number of red nodes, in order to achieve a maximal tree height with a minimal black height. Besides this path all other nodes have to be black. If a node is taken off this tree it either loses height or some RB property.
The RB tree of height formula_23 with red root is minimal. This is in agreement with
A minimal RB tree (RB"h" in figure 4) of height formula_25 has two children of different height. The higher child is also a minimal RB tree, containing also a longest path which defines its height it has formula_26 nodes and the black height formula_27 The other child is a perfect binary tree of (black) height formula_28 having formula_29 black nodes. Then the number of nodes is by induction
The graph of the function formula_30 is convex and piecewise linear with breakpoints at formula_31 where formula_32 The function has been tabulated as formula_33 A027383("h"–1) for formula_34 
The inequality formula_36 leads to formula_37 which for odd formula_4 leads to
which yields
for all formula_4 (odd and even). So formula_4 is in the interval
with formula_43 as the number of nodes.
A red–black tree with formula_2 nodes (keys) has a height formula_45
Set operations and bulk operations.
In addition to the single-element insert, delete and lookup operations, several set operations have been defined on red–black trees: union, intersection and set difference. Then fast "bulk" operations on insertions or deletions can be implemented based on these set functions. These set operations rely on two helper operations, "Split" and "Join". With the new operations, the implementation of red–black trees can be more efficient and highly-parallelizable. This implementation allows a red root.
The join algorithm is as follows:
 function joinRightRB(TL, k, TR)
 if r(TL)=⌊r(TL)/2⌋×2:
 return Node(TL,⟨k,red⟩,TR)
 else 
 (L',⟨k',c'⟩,R')=expose(TL)
 T'=Node(L',⟨k',c'⟩,joinRightRB(R',k,TR)
 if (c'=black) and (T'.right.color=T'.right.right.color=red):
 T'.right.right.color=black;
 return rotateLeft(T')
 else return T'
 function joinLeftRB(TL, k, TR)
 /* symmetric to joinRightRB */
 function join(TL, k, TR)
 if ⌊r(TL)/2⌋&gt;⌊r(TR)/2⌋×2:
 T'=joinRightRB(TL,k,TR)
 if (T'.color=red) and (T'.right.color=red):
 T'.color=black
 return T'
 else if ⌊r(TR)/2⌋&gt;⌊r(TL)/2⌋×2
 /* symmetric */
 else if (TL.color=black) and (TR.color=black)
 Node(TL,⟨k,red⟩,TR)
 else
 Node(TL,⟨k,black⟩,TR)
Here formula_47 of a node formula_48 means twice the black height of a black node, and the twice the black height of a red node. expose(v)=(l,⟨k,c⟩,r) means to extract a tree node formula_48’s left child formula_50, the key of the node formula_51, the color of the node formula_52 and the right child formula_53. Node(l,⟨k,c⟩,r) means to create a node of left child formula_50, key formula_51, color formula_52 and right child formula_53.
The split algorithm is as follows:
 function split(T, k)
 if (T = nil) return (nil, false, nil)
 (L,(m,c),R) = expose(T)
 if (k = m) return (L, true, R)
 if (k &lt; m) 
 (L',b,R') = split(L, k)
 return (L',b,join(R',m,R))
 if (k&gt;m) 
 (L',b,R') = split(R, k)
 return (join(L,m,L'),b,R))
The union of two red–black trees and representing sets and , is a red–black tree that represents . The following recursive function computes this union:
 function union(t1, t2):
 if t1 = nil:
 return t2
 if t2 = nil:
 return t1
 t&lt;, t&gt; ← split t2 on t1.root
 return join(t1.root, union(left(t1), t&lt;), union(right(t1), t&gt;))
Here, "Split" is presumed to return two trees: one holding the keys less its input key, one holding the greater keys. (The algorithm is non-destructive, but an in-place destructive version exists as well.)
The algorithm for intersection or difference is similar, but requires the "Join2" helper routine that is the same as "Join" but without the middle key. Based on the new functions for union, intersection or difference, either one key or multiple keys can be inserted to or deleted from the red–black tree. Since "Split" calls "Join" but does not deal with the balancing criteria of red–black trees directly, such an implementation is usually called the "join-based" implementation.
The complexity of each of union, intersection and difference is formula_58 for two red–black trees of sizes formula_59 and formula_60. This complexity is optimal in terms of the number of comparisons. More importantly, since the recursive calls to union, intersection or difference are independent of each other, they can be executed in parallel with a parallel depth formula_61. When formula_62, the join-based implementation has the same computational directed acyclic graph (DAG) as single-element insertion and deletion if the root of the larger tree is used to split the smaller tree.
Parallel algorithms.
Parallel algorithms for constructing red–black trees from sorted lists of items can run in constant time or formula_63 time, depending on the computer model, if the number of processors available is asymptotically proportional to the number formula_2 of items where formula_65. Fast search, insertion, and deletion parallel algorithms are also known.
The join-based algorithms for red–black trees are parallel for bulk operations, including union, intersection, construction, filter, map-reduce, and so on.
Parallel bulk operations.
Basic operations like insertion, removal or update can be parallelized by defining operations that process bulks of multiple elements. It is also possible to process bulks with several basic operations, for example bulks may contain elements to insert and also elements to remove from the tree.
The algorithms for bulk operations aren’t just applicable to the red–black tree, but can be adapted to other sorted sequence data structures as well, like the 2–3 tree, 2–3–4 tree and (a,b)-tree. In the following different algorithms for bulk insert will be explained, but the same algorithms can also be applied to removal and update. Bulk insert is an operation that inserts each element of a sequence formula_66 into a tree formula_67.
Join-based.
This approach can be applied to every sorted sequence data structure that supports efficient join- and split-operations.
The general idea is to split formula_66 and formula_67 in multiple parts and perform the insertions on these parts in parallel.
Note that in Step 3 the constraints for splitting formula_66 assure that in Step 5 the trees can be joined again and the resulting sequence is sorted.
The pseudo code shows a simple divide-and-conquer implementation of the join-based algorithm for bulk-insert.
Both recursive calls can be executed in parallel.
The join operation used here differs from the version explained in this article, instead join2 is used which misses the second parameter k.
 bulkInsert(T, I, k):
 I.sort()
 bulklInsertRec(T, I, k)
 bulkInsertRec(T, I, k):
 if k = 1:
 forall e in I: T.insert(e)
 else
 m := ⌊size(I) / 2⌋
 (T1, _, T2) := split(T, I[m])
 bulkInsertRec(T1, I[0 .. m], ⌈k / 2⌉)
 || bulkInsertRec(T2, I[m + 1 .. size(I) - 1], ⌊k / 2⌋)
 T ← join2(T1, T2)
Execution time.
Sorting formula_66 is not considered in this analysis.
This can be improved by using parallel algorithms for splitting and joining.
In this case the execution time is formula_86.
Pipelining.
Another method of parallelizing bulk operations is to use a pipelining approach.
This can be done by breaking the task of processing a basic operation up into a sequence of subtasks.
For multiple basic operations the subtasks can be processed in parallel by assigning each subtask to a separate processor.
Execution time.
Sorting formula_66 is not considered in this analysis.
Also, formula_117 is assumed to be smaller than formula_118, otherwise it would be more sufficient to construct the resulting tree from scratch.
Popular culture.
A red–black tree was referenced correctly in an episode of "Missing" as noted by Robert Sedgewick in one of his lectures:
External links.
[[Category:1972 in computing]]
[[Category:Articles containing proofs]]
[[Category:Articles with example C code]]
[[Category:Binary trees]]
[[Category:Search trees]]

