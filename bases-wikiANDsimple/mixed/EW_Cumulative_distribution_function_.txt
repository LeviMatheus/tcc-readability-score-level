
In probability theory and statistics, the cumulative distribution function (CDF) of a real-valued random variable formula_1, or just distribution function of formula_1, evaluated at formula_3, is the probability that formula_1 will take a value less than or equal to formula_3.
Every probability distribution supported on the real numbers, discrete or "mixed" as well as continuous, is uniquely identified by an upwards continuous monotonic increasing cumulative distribution function formula_6 satisfying formula_7 and formula_8.
In the case of a scalar continuous distribution, it gives the area under the probability density function from minus infinity to formula_3. Cumulative distribution functions are also used to specify the distribution of multivariate random variables.
Definition.
The cumulative distribution function of a real-valued random variable formula_1 is the function given by
where the right-hand side represents the probability that the random variable formula_1 takes on a value less than or
equal to formula_3. The probability that formula_1 lies in the semi-closed interval formula_14, where formula_15, is therefore
In the definition above, the "less than or equal to" sign, "≤", is a convention, not a universally used one (e.g. Hungarian literature uses "&lt;"), but the distinction is important for discrete distributions. The proper use of tables of the binomial and Poisson distributions depends upon this convention. Moreover, important formulas like Paul Lévy's inversion formula for the characteristic function also rely on the "less than or equal" formulation.
If treating several random variables formula_16 etc. the corresponding letters are used as subscripts while, if treating only one, the subscript is usually omitted. It is conventional to use a capital formula_17 for a cumulative distribution function, in contrast to the lower-case formula_18 used for probability density functions and probability mass functions. This applies when discussing general distributions: some specific distributions have their own conventional notation, for example the normal distribution uses formula_19 and formula_20 instead of formula_17 and formula_18, respectively.
The probability density function of a continuous random variable can be determined from the cumulative distribution function by differentiating using the Fundamental Theorem of Calculus; i.e. given formula_23,
as long as the derivative exists.
The CDF of a continuous random variable formula_1 can be expressed as the integral of its probability density function formula_26 as follows:
In the case of a random variable formula_1 which has distribution having a discrete component at a value formula_29,
If formula_31 is continuous at formula_29, this equals zero and there is no discrete component at formula_29.
Properties.
Every cumulative distribution function formula_31 is non-decreasing and right-continuous, which makes it a càdlàg function. Furthermore,
Every function with these four properties is a CDF, i.e., for every such function, a random variable can be defined such that the function is the cumulative distribution function of that random variable.
If formula_1 is a purely discrete random variable, then it attains values formula_37 with probability formula_38, and the CDF of formula_1 will be discontinuous at the points formula_40:
If the CDF formula_31 of a real valued random variable formula_1 is continuous, then formula_1 is a continuous random variable; if furthermore formula_31 is absolutely continuous, then there exists a Lebesgue-integrable function formula_46 such that
for all real numbers formula_48 and formula_29. The function formula_26 is equal to the derivative of formula_31 almost everywhere, and it is called the probability density function of the distribution of formula_1.
Examples.
As an example, suppose formula_1 is uniformly distributed on the unit interval formula_54.
Then the CDF of formula_1 is given by
Suppose instead that formula_1 takes only the discrete values 0 and 1, with equal probability.
Then the CDF of formula_1 is given by
Suppose formula_1 is exponential distributed. Then the CDF of formula_1 is given by
Here λ &gt; 0 is the parameter of the distribution, often called the rate parameter.
Suppose formula_1 is normal distributed. Then the CDF of formula_1 is given by
Here the parameter formula_66  is the mean or expectation of the distribution; and formula_67  is its standard deviation.
Suppose formula_1 is binomial distributed. Then the CDF of formula_1 is given by
Here formula_71 is the probability of success and the function denotes the discrete probability distribution of the number of successes in a sequence of formula_72 independent experiments, and formula_73 is the "floor" under formula_74, i.e. the greatest integer less than or equal to formula_74.
Derived functions.
Complementary cumulative distribution function (tail distribution).
Sometimes, it is useful to study the opposite question and ask how often the random variable is "above" a particular level. This is called the complementary cumulative distribution function (ccdf) or simply the tail distribution or exceedance, and is defined as
This has applications in statistical hypothesis testing, for example, because the one-sided p-value is the probability of observing a test statistic "at least" as extreme as the one observed. Thus, provided that the test statistic, "T", has a continuous distribution, the one-sided p-value is simply given by the ccdf: for an observed value formula_77 of the test statistic
In survival analysis, formula_79 is called the survival function and denoted formula_80, while the term "reliability function" is common in engineering.
Z-table:
One of the most popular application of cumulative distribution function is standard normal table, also called the unit normal table or Z table, is the value of cumulative distribution function of the normal distribution. It is very useful to use Z-table not only for probabilities below a value which is the original application of cumulative distribution function, but also above and/or between values on standard normal distribution, and it was further extended to any normal distribution.
Folded cumulative distribution.
While the plot of a cumulative distribution often has an S-like shape, an alternative illustration is the folded cumulative distribution or mountain plot, which folds the top half of the graph over,
thus using two scales, one for the upslope and another for the downslope. This form of illustration emphasises the median and dispersion (specifically, the mean absolute deviation from the median) of the distribution or of the empirical results.
Inverse distribution function (quantile function).
If the CDF "F" is strictly increasing and continuous then formula_91 is the unique real number formula_92 such that formula_93. In such a case, this defines the inverse distribution function or quantile function.
Some distributions do not have a unique inverse (for example in the case where formula_94 for all &lt;math&gt;a&lt;x, causing formula_31 to be constant). This problem can be solved by defining, for formula_96, the generalized inverse distribution function:
Some useful properties of the inverse cdf (which are also preserved in the definition of the generalized inverse distribution function) are:
The inverse of the cdf can be used to translate results obtained for the uniform distribution to other distributions.
Empirical distribution function.
The empirical distribution function is an estimate of the cumulative distribution function that generated the points in the sample. It converges with probability 1 to that underlying distribution. A number of results exist to quantify the rate of convergence of the empirical distribution function to the underlying cumulative distribution function.
Multivariate case.
Definition for two random variables.
When dealing simultaneously with more than one random variable the joint cumulative distribution function can also be defined. For example, for a pair of random variables formula_117, the joint CDF formula_118 is given by
where the right-hand side represents the probability that the random variable formula_1 takes on a value less than or
equal to formula_3 and that formula_106 takes on a value less than or
equal to formula_122.
Example of joint cumulative distribution function: 
For two continuous variables "X" and "Y": formula_123 random variables formula_124, the joint CDF formula_125 is given by
Interpreting the formula_126 random variables as a random vector formula_127 yields a shorter notation:
Properties.
Every multivariate CDF is:
The probability that a point belongs to a hyperrectangle is analogous to the 1-dimensional case:
Complex case.
Complex random variable.
The generalization of the cumulative distribution function from real to complex random variables is not obvious because expressions of the form formula_132 make no sense. However expressions of the form formula_133 make sense. Therefore, we define the cumulative distribution of a complex random variables via the joint distribution of their real and imaginary parts:
Complex random vector.
Generalization of yields
as definition for the CDS of a complex random vector formula_136.
Use in statistical analysis.
The concept of the cumulative distribution function makes an explicit appearance in statistical analysis in two (similar) ways. Cumulative frequency analysis is the analysis of the frequency of occurrence of values of a phenomenon less than a reference value. The empirical distribution function is a formal direct estimate of the cumulative distribution function for which simple statistical properties can be derived and which can form the basis of various statistical hypothesis tests. Such tests can assess whether there is evidence against a sample of data having arisen from a given distribution, or evidence against two samples of data having arisen from the same (unknown) population distribution.
Kolmogorov–Smirnov and Kuiper's tests.
The Kolmogorov–Smirnov test is based on cumulative distribution functions and can be used to test to see whether two empirical distributions are different or whether an empirical distribution is different from an ideal distribution. The closely related Kuiper's test is useful if the domain of the distribution is cyclic as in day of the week. For instance Kuiper's test might be used to see if the number of tornadoes varies during the year or if sales of a product vary by day of the week or day of the month.

