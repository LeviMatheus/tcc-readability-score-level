A voltmeter is an instrument used to measure voltage. For instance, a voltmeter can be used to see if there is more electricity left in a battery. The creation of voltmeters became possible when Hans Oersted invented the most simple voltmeter on 1819.
Connecting a voltmeter.
The voltmeter can be connected with two wires to where the voltage is. One wire is the positive one, and the other the negative. With some voltmeters, one "must" make sure that the wires are connected right: the positive connection on the voltmeter to the more positive "part" of the voltage source, and the negative to the more negative "part". This way, the voltmeter is parallel to the electrical circuit.
One should also be careful about handling the connection: If there is a high voltage (many volts), one can get hurt or even killed by touching the metal connections with the voltage directly.
How voltmeters show the voltage.
When the connections are made, the voltmeter shows the voltage. As there is no direct way to assess the voltage, voltmeters are indeed designed as special ammeters which can calculate the voltage by assessing the electrical current and applying the Ohm's law.
There are two kinds of voltmeters. One kind has a needle, or "pointer", that points to a number that tells the number of volts. This is the kind of voltmeter where one has to be careful about making the positive and negative connections right â€” if wrong connections are made, the voltmeter can be damaged.
The second kind of voltmeter shows the numbers in a "digital" way, just like digital clocks and calculators. This kind of voltmeter is not damaged from "wrong" connections; instead, they just show a negative number.
Also, there are two types of voltmeters, based on the current type: some voltmeters are designed for use in direct current (DC) while others are designed for alternating current (AC). Modern voltmeters can work in both currents.
Using the right kind of voltmeter.
All voltmeters have an upper limit, or a "maximum number" of volts they can "handle". If a voltmeter is used for bigger voltages than it was made to "handle", it may damage or destroy it.
To avoid damaging the voltmeter, one could use a voltmeter that can handle a lot of volts, but such voltmeters are often not very precise with small voltages. The best way is to always use a voltmeter that can handle the voltage, but not one that was made for much larger voltages.
Voltages can be either from a direct current or an alternating current. A voltmeter needs to be "prepared", or made for, one of them. If one tries to measure one kind of voltage with a voltmeter made for the other, the voltmeter will either show a wrong voltage, or be destroyed.
Voltmeters with settings, and multimeters.
Because it is important to use the right kind of voltmeter, they are most often made so they can be adjusted to measure all kinds of voltages. Such voltmeters usually have a "knob" or switch that can be set in different ways. If the voltmeter is set in one way, the voltmeter handles voltages up to, for example, 10 volts. If the switch is set in another way, the voltmeter can handle 100 volts, and so on. Inside the voltmeter, the switch usually works by changing resistors in a voltage divider.
In this way, one single voltmeter can be used for a lot of different voltages, large and small. Some modern voltmeters can do this setting all by themselves, one just has to make the connection and not worry about if the voltmeter can handle the voltage. It will automatically find a setting that can handle it.
Today, a voltmeter is usually part of a multimeter, an instrument that can work as both a voltmeter, an ammeter, and usually a few more measuring instruments. These also have switces, used to "tell" the multimeter to "be a voltmeter".
Multimeters often have more than two connections, and part of "telling" the multimeter what to measure (that is, whether to be a voltmeter or an amperemeter) is done by choosing the right two connections. This is explained in the manual for the multimeter, and often shown next to the connection points.
Use of amplifiers for sensitive measurement of voltage.
The first type of voltmeters show the voltage using a needle or "pointer" pointing to the number of volts. These voltmeters take energy from the thing being measured to move the needle. Some very weak voltage sources might not have enough energy to move the needle to the right voltage. In that case, such a voltmeter shows too few volts. The voltmeter is not sensitive enough.
One solution to the above problem is to make the needle use as little energy as possible to move. However, there is a limit to how sensitive this kind of voltmeter can be. When vacuum tubes and transistors were invented, it became possible to build electronic amplifiers. By using an amplifier, a voltmeter can measure "very" small voltages from "very" weak sources. Modern voltmeters and multimeters normally have this kind of amplifier in them.
